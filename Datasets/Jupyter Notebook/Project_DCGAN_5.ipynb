{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_DCGAN5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej-INli4K2Z3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alxms50VZW4s"
      },
      "source": [
        "# importing required packages\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from IPython import display\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wf95-OwZZ-n"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data2.txt\",header=None)\n",
        "index = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/index.txt\",header=None)\n",
        "data2 = data2.to_numpy()\n",
        "index = index.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS6-5NjCZdz9"
      },
      "source": [
        "print(data.shape)\n",
        "print(index.shape)\n",
        "train_dataset = np.reshape(data,(-1,28,28,1))\n",
        "print(train_dataset.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr9rdF7hZicb"
      },
      "source": [
        "print(train_dataset[2].shape)\n",
        "train_imgs = train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBb28BcvZki3"
      },
      "source": [
        "N_Imag = 49920\n",
        "Batch_Size = 12\n",
        "noise_d = 100\n",
        "train_set = tf.data.Dataset.from_tensor_slices(train_imgs).shuffle(N_Imag).batch(Batch_Size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVxz4ROuZov-"
      },
      "source": [
        "# Architecture for Generator\n",
        "\n",
        "def generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    # First Layer (from noise 100x1 to 7*7*256)\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(noise_d,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) \n",
        "\n",
        "    # Second Layer (1st Deconvolution)\n",
        "    model.add(layers.Conv2DTranspose(128, (5,5), strides=(1,1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    # Thirds Layer (2nd Deconvolution)\n",
        "    model.add(layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    # Forth Layer (3rd Deconvolution)\n",
        "    model.add(layers.Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model\n",
        "\n",
        "G = generator_model()\n",
        "# Making Noise !!\n",
        "noise = tf.random.normal([1, 100])\n",
        "# Feeding the noise to the generator\n",
        "generated_img = G(noise, training=False)\n",
        "# Plotting the (not-so-fancy) output \n",
        "plt.imshow(generated_img[0,:,:,0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr7pvYp9arte"
      },
      "source": [
        "G.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMG9tpLaazUq"
      },
      "source": [
        "plot_model(G, to_file='model_gen.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ_1c3e4a1sI"
      },
      "source": [
        "def discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    # First 2D Conv Layer\n",
        "    model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[28,28,1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    # Second 2D Conv Layer\n",
        "    model.add(layers.Conv2D(128,(5,5), strides=(2,2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    # Flatten the output of the second layer\n",
        "    model.add(layers.Flatten())\n",
        "    # Dense layer for scalar output\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model\n",
        "\n",
        "D = discriminator_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWcSLLBia4mG"
      },
      "source": [
        "print(D.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXfN8cKMa43Y"
      },
      "source": [
        "plot_model(D, to_file='model_dis.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66gqu-eaa-YZ"
      },
      "source": [
        "# Defining Binary-Cross-Entropy loss function instance \n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "# Discriminator Loss Function\n",
        "def discriminator_loss(true_output, fake_output):\n",
        "    T = cross_entropy(tf.ones_like(true_output), true_output)\n",
        "    F = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    D_loss = F + T\n",
        "    return D_loss\n",
        "# Generator Loss Function\n",
        "def generator_loss(fake_output):\n",
        "    F = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "    return F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zheg9LWbB1K"
      },
      "source": [
        "G_Opt = tf.keras.optimizers.Adam(1e-4)\n",
        "D_Opt = tf.keras.optimizers.Adam(1e-4)\n",
        "Epochs = 50 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHFP8bELbGLF"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([Batch_Size, noise_d])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        # Making a Noise generated image through the Generator\n",
        "        generated_img = G(noise, training=True) \n",
        "        # True and Fake outputs from Discriminator\n",
        "        T_out = D(images, training=True)\n",
        "        F_out = D(generated_img, training=True)\n",
        "        # Generator Loss\n",
        "        G_Loss = generator_loss(F_out)\n",
        "        # Discriminator Loss\n",
        "        D_Loss = discriminator_loss(T_out, F_out)\n",
        "    # Gradient Computation\n",
        "    grad_gen = gen_tape.gradient(G_Loss, G.trainable_variables)\n",
        "    grad_disc = disc_tape.gradient(D_Loss, D.trainable_variables)\n",
        "    # Updating the weights\n",
        "    G_Opt.apply_gradients(zip(grad_gen, G.trainable_variables))\n",
        "    D_Opt.apply_gradients(zip(grad_disc, D.trainable_variables))\n",
        "    # Returning the Losses\n",
        "    return tf.cast(G_Loss, tf.float32),tf.cast(D_Loss, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnN0oIV9bGxg"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "\n",
        "    G_Loss_History = []\n",
        "    D_Loss_History = []\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        G_Losses = []\n",
        "        D_Losses = []\n",
        "        for img_batch in dataset:\n",
        "            G_Loss, D_Loss = train_step(img_batch)\n",
        "            # Saving the Loss values in an array for the entire dataset\n",
        "            G_Losses.append(G_Loss)\n",
        "            D_Losses.append(D_Loss)\n",
        "            \n",
        "        # History of all Loss Function Arrays\n",
        "        G_Loss_History.append(G_Losses)\n",
        "        D_Loss_History.append(D_Losses)\n",
        "\n",
        "    # Returning Errors\n",
        "    return G_Loss_History, D_Loss_History"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTmkDl_YbJPQ"
      },
      "source": [
        "G_Loss_History, D_Loss_History = train(train_set, Epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F9HHl28bNll"
      },
      "source": [
        "G_Loss = []\n",
        "D_Loss = []\n",
        "for i in range(Epochs):\n",
        "  G_Loss_i = G_Loss_History[i]\n",
        "  Mean = sum(G_Loss_i)/len(G_Loss_i)\n",
        "  G_Loss.append(Mean.numpy())\n",
        "  D_Loss_i = D_Loss_History[i]\n",
        "  Mean = sum(D_Loss_i)/len(D_Loss_i)\n",
        "  D_Loss.append(Mean.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0tC0u0kbPV_"
      },
      "source": [
        "fig, ax1 = plt.subplots(figsize=(20, 5))\n",
        "ax1.plot(G_Loss, 'b-')\n",
        "ax1.set_ylabel('Generator Loss', color='b')\n",
        "ax1.set_xlabel('Epochs',color='k')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(D_Loss, 'r-')\n",
        "ax2.set_ylabel('Discriminator Loss', color='r')\n",
        "plt.title('Batch Size = 32')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-3rse08bS1_"
      },
      "source": [
        "seed = tf.random.normal([Batch_Size, noise_d])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDRqILDTbbYi"
      },
      "source": [
        "predictions = G(seed, training=False)\n",
        "predictions = predictions.numpy()\n",
        "predictions = np.reshape(predictions,(28,28))\n",
        "print(predictions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_wWeysCbbnK"
      },
      "source": [
        "df = pd.DataFrame(data=predictions)\n",
        "df.to_csv(\"./GAN5_out.csv\", sep=',',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}