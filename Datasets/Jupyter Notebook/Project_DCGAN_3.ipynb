{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_DCGAN_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAjibwEPCQJQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "a688c4b8-2081-4173-82ee-9c9c6402377a"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d1d5f2f639c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwKtt3XN2zxi"
      },
      "source": [
        "# importing required packages\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from IPython import display\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMFCsmk525cB"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TgA2jNp26zp"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data.txt\",header=None)\n",
        "index = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/index.txt\",header=None)\n",
        "data = data.to_numpy()\n",
        "index = index.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE5Z3Oq128Up"
      },
      "source": [
        "print(data.shape)\n",
        "print(index.shape)\n",
        "train_dataset = np.reshape(data,(-1,720,16,1))\n",
        "print(train_dataset.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlch7NKx3AMo"
      },
      "source": [
        "print(train_dataset[2].shape)\n",
        "train_imgs = train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jymwiXG73CgW"
      },
      "source": [
        "N_Imag = 3120\n",
        "Batch_Size = 12\n",
        "noise_d = 500\n",
        "train_set = tf.data.Dataset.from_tensor_slices(train_imgs).shuffle(N_Imag).batch(Batch_Size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuuAAwPOlW_f"
      },
      "source": [
        "# Architecture for Generator\n",
        "\n",
        "def generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    # First Layer (from noise 200x1 to 7*7*256)\n",
        "    model.add(layers.Dense(2*90*64, use_bias=False, input_shape=(noise_d,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((90, 2, 64)))\n",
        "    assert model.output_shape == (None, 90, 2, 64) \n",
        "\n",
        "    # Second Layer (1st Deconvolution)\n",
        "    model.add(layers.Conv2DTranspose(128, (20,2), strides=(1,1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 90, 2, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Second Layer (1st Deconvolution)\n",
        "    model.add(layers.Conv2DTranspose(64, (20,5), strides=(2,2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 180, 4, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    # Thirds Layer (2nd Deconvolution)\n",
        "    model.add(layers.Conv2DTranspose(32, (20,10), strides=(2,2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 360, 8, 32)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (20,2), strides=(2,2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 720, 16, 1)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Forth Layer (3rd Deconvolution)\n",
        "    #model.add(layers.Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh'))\n",
        "    #assert model.output_shape == (None, 16, 720, 1)\n",
        "\n",
        "    return model\n",
        "\n",
        "G = generator_model()\n",
        "# Making Noise !!\n",
        "noise = tf.random.normal([1, noise_d])\n",
        "# Feeding the noise to the generator\n",
        "generated_img = G(noise, training=False)\n",
        "# Plotting the (not-so-fancy) output \n",
        "#plt.imshow(generated_img[0,:,:,0], cmap='gray')\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.imshow(np.abs(generated_img[0,:,:,0]),cmap='Greys')\n",
        "plt.xlabel('channels')\n",
        "plt.ylabel('samples')\n",
        "ax.set_aspect(0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B8H5zeSlo_c"
      },
      "source": [
        "G.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZyRgGld4Kbc"
      },
      "source": [
        "plot_model(G, to_file='model_gen.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbX2bmZgrD72"
      },
      "source": [
        "def discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    # First 2D Conv Layer\n",
        "    model.add(layers.Conv2D(64, (10,2), strides=(2,2), padding='same', input_shape=[720,16,1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    # Second 2D Conv Layer\n",
        "    model.add(layers.Conv2D(128,(10,2), strides=(2,2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    # Flatten the output of the second layer\n",
        "    model.add(layers.Flatten())\n",
        "    # Dense layer for scalar output\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model\n",
        "\n",
        "D = discriminator_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9oM1iPCsvkZ"
      },
      "source": [
        "D.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pZTsJd1sy9_"
      },
      "source": [
        "plot_model(D, to_file='model_dis.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F8TnBH2uilI"
      },
      "source": [
        "# Defining Binary-Cross-Entropy loss function instance \n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "# Discriminator Loss Function\n",
        "def discriminator_loss(true_output, fake_output):\n",
        "    T = cross_entropy(tf.ones_like(true_output), true_output)\n",
        "    F = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    D_loss = F + T\n",
        "    return D_loss\n",
        "# Generator Loss Function\n",
        "def generator_loss(fake_output):\n",
        "    F = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "    return F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V5rGK47ukt5"
      },
      "source": [
        "# Defining the number of epochs before the training\n",
        "Epochs = 500\n",
        "G_Opt = tf.keras.optimizers.Adam(1e-3)\n",
        "D_Opt = tf.keras.optimizers.Adam(1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibGrteyNEUlu"
      },
      "source": [
        "def input_for_GAN(Batch_Size, ND ,Train_Images, n_Imag):\n",
        "  N = np.random.randint(0,n_Imag,Batch_Size)\n",
        "  M = np.random.randint(0,16,Batch_Size)\n",
        "  noise = np.random.normal(size=(Batch_Size, ND))\n",
        "  j = 0\n",
        "  for i in N:\n",
        "      noise[j,0:99] = Train_Images[i,0:99, M[j],0]\n",
        "      j = j + 1\n",
        "  noise = np.reshape(noise,(Batch_Size,ND,1))\n",
        "  return noise\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVHfA2r0QJWt"
      },
      "source": [
        "noise = input_for_GAN(Batch_Size, noise_d, train_imgs, N_Imag)\n",
        "print(noise.shape)\n",
        "#noise = np.reshape(noise,(Batch_Size,noise_d,1))\n",
        "#print(noise.shape)\n",
        "#noise = np.transpose(noise)\n",
        "#print(noise.shape)\n",
        "#noise = np.reshape(noise,(Batch_Size,noise_d,-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWBseInzutQB"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = input_for_GAN(Batch_Size, noise_d, train_imgs, N_Imag)\n",
        "    noise = tf.convert_to_tensor(noise)\n",
        "    noise = tf.reshape(noise,([Batch_Size,noise_d]))\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        # Making a Noise generated image through the Generator\n",
        "        generated_img = G(noise, training=True) \n",
        "        # True and Fake outputs from Discriminator\n",
        "        T_out = D(images, training=True)\n",
        "        F_out = D(generated_img, training=True)\n",
        "        # Generator Loss\n",
        "        G_Loss = generator_loss(F_out)\n",
        "        # Discriminator Loss\n",
        "        D_Loss = discriminator_loss(T_out, F_out)\n",
        "    # Gradient Computation\n",
        "    grad_gen = gen_tape.gradient(G_Loss, G.trainable_variables)\n",
        "    grad_disc = disc_tape.gradient(D_Loss, D.trainable_variables)\n",
        "    # Updating the weights\n",
        "    G_Opt.apply_gradients(zip(grad_gen, G.trainable_variables))\n",
        "    D_Opt.apply_gradients(zip(grad_disc, D.trainable_variables))\n",
        "    # Returning the Losses\n",
        "    return tf.cast(G_Loss, tf.float32),tf.cast(D_Loss, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boI2SOTTu62a"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "\n",
        "    G_Loss_History = []\n",
        "    D_Loss_History = []\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        start = time.time()\n",
        "        G_Losses = []\n",
        "        D_Losses = []\n",
        "        for img_batch in dataset:\n",
        "            G_Loss, D_Loss = train_step(img_batch)\n",
        "            # Saving the Loss values in an array for the entire dataset\n",
        "            G_Losses.append(G_Loss)\n",
        "            D_Losses.append(D_Loss)\n",
        "            \n",
        "        # History of all Loss Function Arrays\n",
        "        G_Loss_History.append(G_Losses)\n",
        "        D_Loss_History.append(D_Losses)\n",
        "        G_Mean = sum(G_Losses)/len(G_Losses)\n",
        "        D_Mean = sum(D_Losses)/len(D_Losses)\n",
        "        print(\"Finished training epoch: \", epoch + 1)\n",
        "        print(\"Generator Loss : \", G_Mean.numpy() , \"Discriminator Loss : \", D_Mean.numpy())\n",
        "\n",
        "\n",
        "    # Returning Errors\n",
        "    return G_Loss_History, D_Loss_History"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5N6ieowDOKW"
      },
      "source": [
        "G_Loss_History, D_Loss_History = train(train_set, Epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqSnLMbRDPWf"
      },
      "source": [
        "G_Loss = []\n",
        "D_Loss = []\n",
        "for i in range(Epochs):\n",
        "  G_Loss_i = G_Loss_History[i]\n",
        "  Mean = sum(G_Loss_i)/len(G_Loss_i)\n",
        "  G_Loss.append(Mean.numpy())\n",
        "  D_Loss_i = D_Loss_History[i]\n",
        "  Mean = sum(D_Loss_i)/len(D_Loss_i)\n",
        "  D_Loss.append(Mean.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGFjMUlJDPUH"
      },
      "source": [
        "fig, ax1 = plt.subplots(figsize=(20, 5))\n",
        "ax1.plot(G_Loss, 'b-')\n",
        "ax1.set_ylabel('Generator Loss', color='b')\n",
        "ax1.set_xlabel('Epochs',color='k')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(D_Loss, 'r-')\n",
        "ax2.set_ylabel('Discriminator Loss', color='r')\n",
        "plt.title('Batch Size = 12')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBXykfD0FHxp"
      },
      "source": [
        "seed = input_for_GAN(Batch_Size, noise_d, train_imgs, N_Imag)\n",
        "seed = tf.convert_to_tensor(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8xGOkhjEu6m"
      },
      "source": [
        "predictions = G(seed, training=False)\n",
        "predictions = predictions.numpy()\n",
        "predictions = np.reshape(predictions,(16,720))\n",
        "print(predictions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ6KYaWeFB7d"
      },
      "source": [
        "df = pd.DataFrame(data=predictions)\n",
        "df.to_csv(\"./GAN3_out.csv\", sep=',',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}